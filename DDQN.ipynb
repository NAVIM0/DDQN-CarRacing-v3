{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf5781e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "7d8694ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T17:44:44.438776Z",
     "start_time": "2025-08-07T17:44:44.429297Z"
    }
   },
   "source": [
    "# Environment imports\n",
    "import math\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tensorflow training imports\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Training monitoring imports\n",
    "import datetime, os\n",
    "from collections import deque\n",
    "import time\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "79252aa5",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b83b1e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T17:44:44.455040Z",
     "start_time": "2025-08-07T17:44:44.447609Z"
    }
   },
   "source": [
    "############################## CONFIGURATION ##################################\n",
    "# Prevent tensorflow from allocating the all of GPU memory\n",
    "# From: https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\n",
    "\n",
    "\n",
    "# 1) Mixed‑precision to leverage Tensor‑core speedups\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# 2) Enable XLA (JIT) compilation for fused kernels\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "\n",
    "\n",
    "GPUs = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in GPUs:\n",
    "    tf.config.experimental.set_memory_growth( gpu, True )   # set memory growth option\n",
    "\n",
    "# Creates a virtual display for OpenAI gym ( to support running from headless servers)\n",
    "# pyvirtualdisplay.Display( visible=0, size=(720, 480) ).start()\n",
    "\n",
    "# Where are models saved? How frequently e.g. every x1 episode?\n",
    "MODEL_TYPE              = \"DDQN_NN\"\n",
    "TIMESTAMP               = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "MODEL_DIR               = f\"./model/{MODEL_TYPE}/\"\n",
    "\n",
    "# Setup Reward Dir\n",
    "REWARD_DIR              = f\"rewards/{MODEL_TYPE}/\"\n",
    "\n",
    "# Training params\n",
    "RENDER                  = True\n",
    "PLOT_RESULTS            = True     # plotting reward and epsilon vs epsiode (graphically) NOTE: THIS WILL PAUSE TRAINING AT PLOT EPISODE!\n",
    "EPISODES                = 5000      # training episodes\n",
    "SAVE_TRAINING_FREQUENCY = 100       # save model every n episodes\n",
    "SKIP_FRAMES             = 2         # skip n frames between batches\n",
    "TARGET_UPDATE_STEPS     = 2         # update target action value network every n EPISODES\n",
    "MAX_PENALTY             = -30       # min score before env reset\n",
    "BATCH_SIZE              = 64        # number for batch fitting\n",
    "CONSECUTIVE_NEG_REWARD  = 25        # number of consecutive negative rewards before terminating episode\n",
    "STEPS_ON_GRASS          = 20        # How many steps can car be on grass for (steps == states)\n",
    "REPLAY_BUFFER_MAX_SIZE  = 150000    # threshold memory limit for replay buffer (old version was 10000)\n",
    "\n",
    "# Steering history for smoothing throttle/brake rewards\n",
    "STEER_BUFFER_LEN = 5\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_MIN   = 0.01\n",
    "\n",
    "# compute decay rate so ε decays from 1.0→0.01 over EPISODES:\n",
    "EPSILON_DECAY = math.exp(math.log(EPSILON_MIN/EPSILON_START) / EPISODES)\n",
    "# Testing params\n",
    "PRETRAINED_PATH = os.path.join(MODEL_DIR, \"model.weights.h5\")\n",
    "TEST                    = True      # true = testing, false = training"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "19a42f2a",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a40f429",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T17:44:44.470548Z",
     "start_time": "2025-08-07T17:44:44.460559Z"
    }
   },
   "source": [
    "def convert_greyscale( state ):\n",
    "    \"\"\"Take input state and convert to greyscale. Check if road is visible in frame.\"\"\"\n",
    "    global on_grass_counter\n",
    "\n",
    "    x, y, _ = state.shape\n",
    "    cropped = state[ 0:int( 0.85*y ) , 0:x ]\n",
    "    mask = cv2.inRange( cropped,  np.array([100, 100, 100]),  # dark_grey\n",
    "                                  np.array([150, 150, 150]))  # light_grey\n",
    "\n",
    "    # Create greyscale then normalize array to reduce complexity for neural network\n",
    "    gray = cv2.cvtColor( state, cv2.COLOR_BGR2GRAY )\n",
    "    gray = gray.astype(float)\n",
    "    gray_normalised = gray / 255.0\n",
    "\n",
    "    # check if car is on grass\n",
    "    xc = int(x / 2)\n",
    "    grass_mask = cv2.inRange(   state[67:76 , xc-2:xc+2],\n",
    "                                np.array([50, 180, 0]),\n",
    "                                np.array([150, 255, 255]))\n",
    "\n",
    "    # If on grass for x5 frames or more then trigger True!\n",
    "    on_grass_counter = on_grass_counter+1 if np.any(grass_mask==255) and \"on_grass_counter\" in globals() else 0\n",
    "    if on_grass_counter > STEPS_ON_GRASS:\n",
    "        on_grass = True\n",
    "        on_grass_counter = 0\n",
    "    else: on_grass = False\n",
    "\n",
    "    # returns [ greyscale image, T/F of if road is visible, is car on grass bool ]\n",
    "    return [ np.expand_dims( gray_normalised, axis=2 ), np.any(mask== 255), on_grass ]\n",
    "\n",
    "\n",
    "def calculate_off_far(state):\n",
    "    \"\"\"\n",
    "    Compute lateral offset of the road centroid from center,\n",
    "    using only the upper 40% of the frame to capture upcoming turns.\n",
    "    \"\"\"\n",
    "    h, w, _ = state.shape\n",
    "\n",
    "    # Only take the top 40% of the image (0 → 0.4*h):\n",
    "    bottom = int(0.4 * h)\n",
    "    crop   = state[0:bottom, 0:w]\n",
    "\n",
    "    # Mask out the gray track pixels:\n",
    "    mask = cv2.inRange(\n",
    "        crop,\n",
    "        np.array([100,100,100]),\n",
    "        np.array([150,150,150])\n",
    "    )\n",
    "\n",
    "    # Compute image moments of that mask:\n",
    "    M = cv2.moments(mask)\n",
    "    if M[\"m00\"] != 0:\n",
    "        # centroid x = m10/m00\n",
    "        cx = M[\"m10\"] / M[\"m00\"]\n",
    "        # normalize to –1…+1 around image center:\n",
    "        off_far = (cx - (w/2)) / (w/2)\n",
    "        return float(off_far)\n",
    "    else:\n",
    "        # no track pixels detected ⇒ treat as a very sharp turn\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "\n",
    "def plot_agent(data, path):\n",
    "    data = np.array(data)\n",
    "    \"\"\"\n",
    "    Plot agent's training progress from a CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    episodes = np.arange(len(data))\n",
    "\n",
    "    # Plotting\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "\n",
    "    # Plot 1: Reward per episode\n",
    "    ax1.plot(episodes, data[:,[0]], label=\"Total Reward\", color='green', alpha=0.6)\n",
    "    ax1.set_ylabel(\"Reward\")\n",
    "    ax1.set_title(\"Agent Reward per Episode\")\n",
    "    ax1.grid(True)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot 2: Epsilon per episode\n",
    "    ax2.plot(episodes, data[:,[1]], label=\"Epsilon\", color='orange')\n",
    "    ax2.set_ylabel(\"Epsilon\")\n",
    "    ax2.set_xlabel(\"Episode\")\n",
    "    ax2.set_title(\"Epsilon Decay\")\n",
    "    ax2.grid(True)\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path)\n",
    "    plt.close()"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "ee9e6828",
   "metadata": {},
   "source": [
    "# Main Agent "
   ]
  },
  {
   "cell_type": "code",
   "id": "ed5718b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T17:44:44.495088Z",
     "start_time": "2025-08-07T17:44:44.474417Z"
    }
   },
   "source": [
    "class DDQN_Agent:\n",
    "    def __init__(self,\n",
    "                 action_space=[(-1, 1, 0.2), (0, 1, 0.2), (1, 1, 0.2),\n",
    "                               (-1, 1, 0),   (0, 1,   0),   (1, 1,   0),\n",
    "                               (-1, 0, 0.2), (0, 0, 0.2), (1, 0, 0.2),\n",
    "                               (-1, 0,   0), (0, 0,   0), (1, 0,   0)],\n",
    "                 gamma=0.99,\n",
    "                 epsilon=EPSILON_START,\n",
    "                 epsilon_min=EPSILON_MIN,\n",
    "                 epsilon_decay=EPSILON_DECAY,\n",
    "                 learning_rate=0.0001\n",
    "                 ):\n",
    "        self.action_space   = action_space\n",
    "        self.D              = deque(maxlen=REPLAY_BUFFER_MAX_SIZE)\n",
    "        self.gamma          = gamma\n",
    "        self.epsilon        = epsilon\n",
    "        self.epsilon_min    = epsilon_min\n",
    "        self.epsilon_decay  = epsilon_decay\n",
    "        self.learning_rate  = learning_rate\n",
    "        self.model          = self.build_model()\n",
    "        self.target_model   = tf.keras.models.clone_model(self.model)\n",
    "        self.train_step     = 0\n",
    "\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(tf.keras.Input(shape=(96, 96, 1)))\n",
    "        model.add(Conv2D(6, (7, 7), strides=3, activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(12, (4, 4), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(216, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(len(self.action_space), activation=None))\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.Huber(),\n",
    "            optimizer=Adam(learning_rate=self.learning_rate, epsilon=1e-7)\n",
    ")\n",
    "        return model\n",
    "\n",
    "    def update_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        self.D.append((state, action, reward, new_state, done))\n",
    "\n",
    "    def choose_action(self, state, best=False, _random=False):\n",
    "        state_batch = np.expand_dims(state, axis=0)\n",
    "        if _random:\n",
    "            idx = random.randrange(len(self.action_space))\n",
    "        elif best:\n",
    "            q = self.model.predict(state_batch, verbose=0)[0]\n",
    "            idx = np.argmax(q)\n",
    "        else:\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                idx = random.randrange(len(self.action_space))\n",
    "            else:\n",
    "                q = self.model.predict(state_batch, verbose=0)[0]\n",
    "                idx = np.argmax(q)\n",
    "        return self.action_space[idx]\n",
    "\n",
    "\n",
    "    def experience_replay(self):\n",
    "        \"\"\"\n",
    "        Vectorized Double‑DQN update over a minibatch.\n",
    "        \"\"\"\n",
    "        if len(self.D) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # 1) Sample and unpack\n",
    "        minibatch       = random.sample(self.D, BATCH_SIZE)\n",
    "        states          = np.stack([t[0] for t in minibatch])\n",
    "        action_idxs     = np.array([t[1] for t in minibatch])\n",
    "        rewards_arr     = np.array([t[2] for t in minibatch], dtype=np.float32)\n",
    "        next_states     = np.stack([t[3] for t in minibatch])\n",
    "        done_masks      = np.array([t[4] for t in minibatch], dtype=np.bool_)\n",
    "\n",
    "        # 2) Batch Q‑value predictions\n",
    "        q_current     = self.model.predict(states,      verbose=0)\n",
    "        q_next_online = self.model.predict(next_states, verbose=0)\n",
    "        q_next_target = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        # 3) Compute targets using vectorized Double‑DQN :contentReference[oaicite:2]{index=2}\n",
    "        #   best_next_actions[i] = argmax_a Q_online(next_states[i], a)\n",
    "        best_next_actions = np.argmax(q_next_online, axis=1)\n",
    "        #   corresponding Q_target values:\n",
    "        best_next_q = q_next_target[np.arange(BATCH_SIZE), best_next_actions]\n",
    "        #   TD target: r + γ * Q_target * (1 - done)\n",
    "        td_targets = rewards_arr + self.gamma * best_next_q * (~done_masks)\n",
    "\n",
    "        # 4) Replace only the taken-action Q‑values in q_current\n",
    "        q_current[np.arange(BATCH_SIZE), action_idxs] = td_targets\n",
    "\n",
    "        # 5) Train on this batch using Huber loss for robustness :contentReference[oaicite:3]{index=3}\n",
    "        # (Assuming your model was compiled with Huber; if not, switch loss to tf.keras.losses.Huber())\n",
    "        self.model.fit(states,\n",
    "                       q_current,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       epochs=1,\n",
    "                       verbose=0)\n",
    "\n",
    "        # 6) Periodically sync target network\n",
    "        self.train_step += 1\n",
    "        if self.train_step % TARGET_UPDATE_STEPS == 0:\n",
    "            self.update_model()\n",
    "\n",
    "\n",
    "    def save(self, data):\n",
    "        name = f'data_{TIMESTAMP}'\n",
    "        if not os.path.exists(MODEL_DIR):\n",
    "            os.makedirs(MODEL_DIR)\n",
    "        self.target_model.save_weights(MODEL_DIR + \"model.weights.h5\")\n",
    "        if not os.path.exists(REWARD_DIR):\n",
    "            os.makedirs(REWARD_DIR)\n",
    "        filepath = os.path.join(REWARD_DIR, name + \".csv\")\n",
    "        plotpath = os.path.join(REWARD_DIR, name + \".jpg\")\n",
    "        plot_agent(data, plotpath)\n",
    "        np.savetxt(filepath, data, delimiter=\",\")\n",
    "\n",
    "    def load(self, name):\n",
    "        self.model.load_weights(name)\n",
    "        self.update_model()\n"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "8265e5e3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f52b10c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T17:44:44.515271Z",
     "start_time": "2025-08-07T17:44:44.502429Z"
    }
   },
   "source": [
    "def train_agent(agent: DDQN_Agent, env: gym.make, episodes: int):\n",
    "    \"\"\"\n",
    "    Train agent with:\n",
    "      - 4-frame temporal smoothing (via average)\n",
    "      - grass penalty, steering-conditioned throttle shaping & steer-jitter penalty\n",
    "      - Double-DQN updates\n",
    "      - Exponential ε-decay applied once per episode\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in tqdm(range(episodes), desc=\"Training\"):\n",
    "        print(f\"[INFO] Starting Episode {episode}\")\n",
    "\n",
    "        # Reset env & preprocess first frame\n",
    "        obs_colour, _ = env.reset()\n",
    "        first_grey, can_see_road, car_on_grass = convert_greyscale(obs_colour)\n",
    "\n",
    "        # 4-frame grey buffer\n",
    "        frame_buffer = deque([first_grey] * 4, maxlen=4)\n",
    "\n",
    "        # Steering‐history buffer (e.g. 7 frames)\n",
    "        steer_buffer = deque([0] * STEER_BUFFER_LEN, maxlen=STEER_BUFFER_LEN)\n",
    "\n",
    "        # Track last steer to penalize jitter\n",
    "        last_steer = 0.0\n",
    "        total_reward    = 0.0\n",
    "        negative_streak = 0\n",
    "        done = False\n",
    "\n",
    "        while (not done) and (total_reward > MAX_PENALTY) and can_see_road:\n",
    "\n",
    "            # 1) Build temporally smoothed NN input\n",
    "            state_input = np.mean(np.stack(frame_buffer, axis=0), axis=0, keepdims=True)\n",
    "\n",
    "            # 2) Agent picks an action\n",
    "            raw_action = agent.choose_action(state_input[0])\n",
    "            steer, gas, brake = raw_action\n",
    "            action = np.array(raw_action, dtype=np.float64)\n",
    "\n",
    "            # 3) Record steering history & penalize rapid changes\n",
    "            steer_buffer.append(steer)\n",
    "            steer_change = abs(steer - last_steer)\n",
    "            last_steer = steer\n",
    "            # small penalty on jitter (only when you rapidly switch steer)\n",
    "            jitter_penalty = 0.005 * steer_change\n",
    "\n",
    "            # 4) Roll out with frame-skip and accumulate base reward\n",
    "            reward_accum = 0.0\n",
    "            for _ in range(SKIP_FRAMES + 1):\n",
    "                next_colour, r, term, trunc, _ = env.step(action)\n",
    "                reward_accum += r\n",
    "                if RENDER:\n",
    "                    env.render()\n",
    "                if term or trunc:\n",
    "                    done = True\n",
    "                    break\n",
    "\n",
    "            # 5) Early termination on too many negatives\n",
    "            negative_streak = (negative_streak + 1) if (reward_accum < 0) else 0\n",
    "            if negative_streak >= CONSECUTIVE_NEG_REWARD:\n",
    "                break\n",
    "\n",
    "            # 6) Preprocess the next frame & update grey buffer\n",
    "            next_grey, can_see_road, car_on_grass = convert_greyscale(next_colour)\n",
    "            frame_buffer.append(next_grey)\n",
    "\n",
    "            # 7) Grass penalty\n",
    "            if car_on_grass:\n",
    "                reward_accum -= 0.12\n",
    "\n",
    "            # 8) Corner‐aware throttle shaping\n",
    "            off_far = calculate_off_far(next_colour)\n",
    "            turniness = sum(abs(s) for s in steer_buffer) / STEER_BUFFER_LEN\n",
    "\n",
    "            # 9) throttle shaping\n",
    "            if abs(off_far) < 0.2 and turniness < 0.2:\n",
    "                # straight road ahead, reward gas\n",
    "                reward_accum += 0.012 * gas\n",
    "            else:\n",
    "                # approaching/in turn, penalize gas\n",
    "                severity = max(turniness, abs(off_far))\n",
    "                reward_accum -= 0.04 * gas * severity\n",
    "\n",
    "\n",
    "            # 2) Brake incentive only when a curve is coming (off_far)\n",
    "            if abs(off_far) > 0.3:\n",
    "                # scaled by how sharp that curve is\n",
    "                reward_accum += 0.015 * brake * abs(off_far)\n",
    "\n",
    "            # 9) Subtract the jitter penalty\n",
    "            reward_accum -= jitter_penalty\n",
    "\n",
    "            # 10) Clip total shaped reward\n",
    "            reward_accum = np.clip(reward_accum, -1.0, 1.0)\n",
    "\n",
    "            # 11) Store and learn\n",
    "            prev_state = state_input[0]\n",
    "            next_state = np.mean(np.stack(frame_buffer, axis=0), axis=0)\n",
    "            action_idx = agent.action_space.index(tuple(raw_action))\n",
    "            agent.store_transition(prev_state, action_idx, reward_accum, next_state, done)\n",
    "            agent.experience_replay()\n",
    "\n",
    "            # 12) Accumulate for logging\n",
    "            total_reward += reward_accum\n",
    "\n",
    "        # Episode done: log & checkpoint\n",
    "        episode_rewards.append([total_reward, agent.epsilon])\n",
    "        print(f\"[INFO] Episode {episode} → Reward: {total_reward:.2f} | ε: {agent.epsilon:.4f}\")\n",
    "\n",
    "        if episode % TARGET_UPDATE_STEPS == 0:\n",
    "            agent.update_model()\n",
    "        if episode % SAVE_TRAINING_FREQUENCY == 0:\n",
    "            agent.save(episode_rewards)\n",
    "\n",
    "        # Exponential ε-decay\n",
    "        agent.epsilon = max(agent.epsilon_min, agent.epsilon * EPSILON_DECAY)\n",
    "\n",
    "    env.close()\n",
    "    return episode_rewards\n"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "7d7facfa",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "id": "508e2be2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T17:44:44.526629Z",
     "start_time": "2025-08-07T17:44:44.522087Z"
    }
   },
   "source": [
    "def test_agent(agent: DDQN_Agent, env: gym.make, model: str, testnum=10):\n",
    "    \"\"\"Test a pretrained model and print out run rewards and total time taken. Quit with ctrl+c.\"\"\"\n",
    "    # Load agent model\n",
    "    agent.load(model)\n",
    "\n",
    "    run_rewards = []\n",
    "    for test in range(testnum):\n",
    "        state_colour, _ = env.reset()\n",
    "        state_grey, _, _ = convert_greyscale(state_colour)\n",
    "\n",
    "        done = False\n",
    "        sum_reward = 0.0\n",
    "        t1 = time.time()\n",
    "        while not done:\n",
    "            action = agent.choose_action(state_grey, best=True)\n",
    "            action = np.array(action, dtype=np.float64)\n",
    "\n",
    "            new_state_colour, r, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            if RENDER:\n",
    "                env.render()\n",
    "\n",
    "            state_grey, _, _ = convert_greyscale(new_state_colour)\n",
    "            sum_reward += r\n",
    "\n",
    "        t1 = time.time() - t1\n",
    "        run_rewards.append([sum_reward, np.nan, t1, np.nan, np.nan, np.nan])\n",
    "        print(f\"[INFO]: Run {test} | Run Reward: {sum_reward:.2f} | Time: {t1:.2f}s\")\n",
    "\n",
    "    rr = [i[0] for i in run_rewards]\n",
    "    rt = [i[2] for i in run_rewards]\n",
    "\n",
    "    r_max = max(rr)\n",
    "    r_min = min(rr)\n",
    "    r_std_dev = np.std(rr)\n",
    "    r_avg = np.mean(rr)\n",
    "    t_avg = np.mean(rt)\n",
    "\n",
    "    run_rewards.append([r_avg, np.nan, t_avg, r_max, r_min, r_std_dev])\n",
    "    print(\n",
    "        f\"[INFO]: Runs {testnum} | Avg Run Reward: {r_avg:.2f} \"\n",
    "        f\"| Avg Time: {t_avg:.2f}s | Max: {r_max:.2f} | Min: {r_min:.2f} | Std Dev: {r_std_dev:.2f}\"\n",
    "    )\n",
    "\n",
    "    # saving test results\n",
    "    if not os.path.exists(f\"test_{REWARD_DIR}\"):\n",
    "        os.makedirs(f\"test_{REWARD_DIR}\")\n",
    "    path = f\"test_{REWARD_DIR}\" + PRETRAINED_PATH.split('/')[-1][:-3] + \"_run_rewards.csv\"\n",
    "    np.savetxt(path, run_rewards, delimiter=\",\")\n",
    "\n",
    "    return [r_avg, np.nan, t_avg, r_max, r_min, r_std_dev]\n"
   ],
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "dd2951b6",
   "metadata": {},
   "source": [
    "main.py"
   ]
  },
  {
   "cell_type": "code",
   "id": "759f39e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-07T17:49:34.718535Z",
     "start_time": "2025-08-07T17:44:44.530619Z"
    }
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    if not TEST:\n",
    "        env = gym.make('CarRacing-v3')\n",
    "        # Train Agent\n",
    "        agent = DDQN_Agent()\n",
    "        train_agent( agent, env, episodes = EPISODES )\n",
    "\n",
    "    else:\n",
    "        # Test Agent\n",
    "        env = gym.make('CarRacing-v3',render_mode='human')\n",
    "        agent = DDQN_Agent()\n",
    "\n",
    "        test_agent( agent, env, model = PRETRAINED_PATH, testnum=5 )"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/navim/CS/AI/RL_FinalPRJ/Code/.venv/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 4 variables whereas the saved optimizer has 26 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n",
      "/Users/navim/CS/AI/RL_FinalPRJ/Code/.venv/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:802: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 22 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Run 0 | Run Reward: 914.00 | Time: 51.43s\n",
      "[INFO]: Run 1 | Run Reward: 907.30 | Time: 57.39s\n",
      "[INFO]: Run 2 | Run Reward: 890.57 | Time: 61.44s\n",
      "[INFO]: Run 3 | Run Reward: 914.30 | Time: 52.90s\n",
      "[INFO]: Run 4 | Run Reward: 886.44 | Time: 62.17s\n",
      "[INFO]: Runs 5 | Avg Run Reward: 902.52 | Avg Time: 57.07s | Max: 914.30 | Min: 886.44 | Std Dev: 11.79\n"
     ]
    }
   ],
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
