{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cf5781e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "id": "7d8694ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T08:55:30.480082Z",
     "start_time": "2025-08-10T08:55:30.477410Z"
    }
   },
   "source": [
    "# Environment imports\n",
    "import math\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Tensorflow training imports\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Training monitoring imports\n",
    "import datetime, os\n",
    "from collections import deque\n",
    "import time\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "79252aa5",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "id": "6b83b1e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T08:55:30.489619Z",
     "start_time": "2025-08-10T08:55:30.485792Z"
    }
   },
   "source": [
    "############################## CONFIGURATION ##################################\n",
    "# Prevent tensorflow from allocating the all of GPU memory\n",
    "# From: https://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\n",
    "\n",
    "\n",
    "# 1) Mixed‑precision to leverage Tensor‑core speedups\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# 2) Enable XLA (JIT) compilation for fused kernels\n",
    "tf.config.optimizer.set_jit(True)\n",
    "\n",
    "GPUs = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in GPUs:\n",
    "    tf.config.experimental.set_memory_growth( gpu, True )   # set memory growth option\n",
    "\n",
    "# Creates a virtual display for OpenAI gym ( to support running from headless servers)\n",
    "# pyvirtualdisplay.Display( visible=0, size=(720, 480) ).start()\n",
    "\n",
    "# Where are models saved? How frequently e.g. every x1 episode?\n",
    "MODEL_TYPE              = \"DDQN_NN\"\n",
    "TIMESTAMP               = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "MODEL_DIR               = f\"./model/{MODEL_TYPE}/\"\n",
    "\n",
    "# Setup Reward Dir\n",
    "REWARD_DIR              = f\"rewards/{MODEL_TYPE}/\"\n",
    "\n",
    "# Training params\n",
    "RENDER                  = True\n",
    "PLOT_RESULTS            = True     # plotting reward and epsilon vs episode (graphically) NOTE: THIS WILL PAUSE TRAINING AT PLOT EPISODE!\n",
    "EPISODES                = 5000      # training episodes\n",
    "SAVE_TRAINING_FREQUENCY = 100       # save model every n episodes\n",
    "SKIP_FRAMES             = 2         # skip n frames between batches\n",
    "TARGET_UPDATE_STEPS     = 2         # update target action value network every n EPISODES\n",
    "MAX_PENALTY             = -30       # min score before env reset\n",
    "BATCH_SIZE              = 64        # number for batch fitting\n",
    "CONSECUTIVE_NEG_REWARD  = 25        # number of consecutive negative rewards before terminating episode\n",
    "STEPS_ON_GRASS          = 20        # How many steps can car be on grass for (steps == states)\n",
    "REPLAY_BUFFER_MAX_SIZE  = 150000    # threshold memory limit for replay buffer (old version was 10000)\n",
    "\n",
    "# Steering history for throttle/brake rewards\n",
    "STEER_BUFFER_LEN = 5\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_MIN   = 0.01\n",
    "\n",
    "# compute decay rate so ε decays from 1.0→0.01 over EPISODES (Exponential Decay):\n",
    "EPSILON_DECAY = math.exp(math.log(EPSILON_MIN/EPSILON_START) / EPISODES)\n",
    "\n",
    "# Testing params\n",
    "PRETRAINED_PATH = os.path.join(MODEL_DIR, \"model.weights.h5\")\n",
    "TEST                    = True      # true = testing, false = training"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "19a42f2a",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a40f429",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T08:55:30.501140Z",
     "start_time": "2025-08-10T08:55:30.494975Z"
    }
   },
   "source": [
    "def convert_greyscale( state ):\n",
    "    \"\"\"Take input state and convert to greyscale. Check if road is visible in frame.\"\"\"\n",
    "    global on_grass_counter\n",
    "\n",
    "    x, y, _ = state.shape\n",
    "    cropped = state[ 0:int( 0.85*y ) , 0:x ]\n",
    "    mask = cv2.inRange( cropped,  np.array([100, 100, 100]),  # dark_grey\n",
    "                                  np.array([150, 150, 150]))  # light_grey\n",
    "\n",
    "    # Create greyscale then normalize array to reduce complexity for neural network\n",
    "    gray = cv2.cvtColor( state, cv2.COLOR_BGR2GRAY )\n",
    "    gray = gray.astype(float)\n",
    "    gray_normalised = gray / 255.0\n",
    "\n",
    "    # check if car is on grass\n",
    "    xc = int(x / 2)\n",
    "    grass_mask = cv2.inRange(   state[67:76 , xc-2:xc+2],\n",
    "                                np.array([50, 180, 0]),\n",
    "                                np.array([150, 255, 255]))\n",
    "\n",
    "    # If on grass for x5 frames or more than trigger True!\n",
    "    on_grass_counter = on_grass_counter+1 if np.any(grass_mask==255) and \"on_grass_counter\" in globals() else 0\n",
    "    if on_grass_counter > STEPS_ON_GRASS:\n",
    "        on_grass = True\n",
    "        on_grass_counter = 0\n",
    "    else: on_grass = False\n",
    "\n",
    "    # returns [ greyscale image, T/F of if road is visible, is car on grass bool ]\n",
    "    return [ np.expand_dims( gray_normalised, axis=2 ), np.any(mask== 255), on_grass ]\n",
    "\n",
    "\n",
    "def calculate_off_far(state):\n",
    "    \"\"\"\n",
    "    Compute x-axis offset of the road up-ahead from center, using only the upper 40% of the frame to predict upcoming turns.\n",
    "    \"\"\"\n",
    "\n",
    "    h, w, _ = state.shape\n",
    "    bottom = int(0.4 * h)\n",
    "    crop   = state[0:bottom, 0:w]\n",
    "\n",
    "    # Mask out the gray track pixels:\n",
    "    mask = cv2.inRange(\n",
    "        crop,\n",
    "        np.array([100,100,100]),\n",
    "        np.array([150,150,150])\n",
    "    )\n",
    "\n",
    "    M = cv2.moments(mask)\n",
    "    if M[\"m00\"] != 0:\n",
    "\n",
    "        cx = M[\"m10\"] / M[\"m00\"]\n",
    "        off_far = (cx - (w/2)) / (w/2)\n",
    "        return float(off_far)\n",
    "    else:\n",
    "        # no track pixels detected ⇒ treat as a very sharp turn\n",
    "        return 1.0\n",
    "\n",
    "\n",
    "\n",
    "def plot_agent(data, path):\n",
    "    data = np.array(data)\n",
    "    \"\"\"\n",
    "    Plot agent's training progress from a CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    episodes = np.arange(len(data))\n",
    "\n",
    "    # Plotting\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6), sharex=True)\n",
    "\n",
    "    # Plot 1: Reward per episode\n",
    "    ax1.plot(episodes, data[:,[0]], label=\"Total Reward\", color='green', alpha=0.6)\n",
    "    ax1.set_ylabel(\"Reward\")\n",
    "    ax1.set_title(\"Agent Reward per Episode\")\n",
    "    ax1.grid(True)\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot 2: Epsilon per episode\n",
    "    ax2.plot(episodes, data[:,[1]], label=\"Epsilon\", color='orange')\n",
    "    ax2.set_ylabel(\"Epsilon\")\n",
    "    ax2.set_xlabel(\"Episode\")\n",
    "    ax2.set_title(\"Epsilon Decay\")\n",
    "    ax2.grid(True)\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path)\n",
    "    plt.close()"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "ee9e6828",
   "metadata": {},
   "source": [
    "# Main Agent "
   ]
  },
  {
   "cell_type": "code",
   "id": "ed5718b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T08:55:30.516762Z",
     "start_time": "2025-08-10T08:55:30.506547Z"
    }
   },
   "source": [
    "class DDQN_Agent:\n",
    "    def __init__(self,\n",
    "                 action_space=[(-1, 1, 0.2), (0, 1, 0.2), (1, 1, 0.2),\n",
    "                               (-1, 1, 0),   (0, 1,   0),   (1, 1,   0),\n",
    "                               (-1, 0, 0.2), (0, 0, 0.2), (1, 0, 0.2),\n",
    "                               (-1, 0,   0), (0, 0,   0), (1, 0,   0)],\n",
    "                 gamma=0.99,                        # discount rate\n",
    "                 epsilon=EPSILON_START,             # exploration rate\n",
    "                 epsilon_min=EPSILON_MIN,           # used by Atari\n",
    "                 epsilon_decay=EPSILON_DECAY,\n",
    "                 learning_rate=0.0001\n",
    "                 ):\n",
    "        self.action_space   = action_space\n",
    "        self.D              = deque(maxlen=REPLAY_BUFFER_MAX_SIZE)\n",
    "        self.gamma          = gamma\n",
    "        self.epsilon        = epsilon\n",
    "        self.epsilon_min    = epsilon_min\n",
    "        self.epsilon_decay  = epsilon_decay\n",
    "        self.learning_rate  = learning_rate\n",
    "\n",
    "        # clone the action value network to make target action value network\n",
    "        self.model          = self.build_model()\n",
    "        self.target_model   = tf.keras.models.clone_model(self.model)\n",
    "        self.train_step     = 0\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Sequential Neural Net with x2 Conv layers, x2 Dense layers using RELU and Huber Loss\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(tf.keras.Input(shape=(96, 96, 1)))\n",
    "        model.add(Conv2D(6, (7, 7), strides=3, activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Conv2D(12, (4, 4), activation='relu'))\n",
    "        model.add(MaxPooling2D((2, 2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(216, activation='relu'))\n",
    "        model.add(Dense(64, activation='relu'))\n",
    "        model.add(Dense(len(self.action_space), activation=None))\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.Huber(),\n",
    "            optimizer=Adam(learning_rate=self.learning_rate, epsilon=1e-7)\n",
    ")\n",
    "        return model\n",
    "\n",
    "    def update_model(self):\n",
    "        \"\"\"Update Target Action Value Network to be equal to Action Value Network\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def store_transition(self, state, action, reward, new_state, done):\n",
    "        \"\"\"Store transition in the replay memory (for replay buffer).\"\"\"\n",
    "        self.D.append((state, action, reward, new_state, done))\n",
    "\n",
    "    def choose_action(self, state, best=False, _random=False):\n",
    "        \"\"\"Take state input and use latest target model to make prediction on best next action; choose it!\"\"\"\n",
    "        state_batch = np.expand_dims(state, axis=0)\n",
    "\n",
    "        if _random:\n",
    "            idx = random.randrange(len(self.action_space))\n",
    "        elif best:\n",
    "            q = self.model.predict(state_batch, verbose=0)[0]\n",
    "            idx = np.argmax(q)\n",
    "        else:\n",
    "            if np.random.rand() < self.epsilon:\n",
    "                idx = random.randrange(len(self.action_space))\n",
    "            else:\n",
    "                q = self.model.predict(state_batch, verbose=0)[0]\n",
    "                idx = np.argmax(q)\n",
    "        return self.action_space[idx]\n",
    "\n",
    "\n",
    "    def experience_replay(self):\n",
    "\n",
    "        if len(self.D) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # 1. Unpack and convert to numpy arrays for vectorized operations\n",
    "        minibatch       = random.sample(self.D, BATCH_SIZE)\n",
    "        states          = np.stack([t[0] for t in minibatch])\n",
    "        action_idxs     = np.array([t[1] for t in minibatch])\n",
    "        rewards_arr     = np.array([t[2] for t in minibatch], dtype=np.float32)\n",
    "        next_states     = np.stack([t[3] for t in minibatch])\n",
    "        done_masks      = np.array([t[4] for t in minibatch], dtype=np.bool_)\n",
    "\n",
    "        # 2. Predict Q-values for current states and next states in one go\n",
    "        q_current     = self.model.predict(states,      verbose=0)\n",
    "        q_next_online = self.model.predict(next_states, verbose=0)\n",
    "        q_next_target = self.target_model.predict(next_states, verbose=0)\n",
    "\n",
    "        # 3. Calculate targets using the DDQN formula\n",
    "        best_next_actions = np.argmax(q_next_online, axis=1)\n",
    "        #   corresponding Q_target values:\n",
    "        best_next_q = q_next_target[np.arange(BATCH_SIZE), best_next_actions]\n",
    "        #   TD target: r + γ * Q_target * (1 - done)\n",
    "        td_targets = rewards_arr + self.gamma * best_next_q * (~done_masks)\n",
    "\n",
    "        q_current[np.arange(BATCH_SIZE), action_idxs] = td_targets\n",
    "\n",
    "        # 4. Train the model on the entire batch\n",
    "        self.model.fit(states,\n",
    "                       q_current,\n",
    "                       batch_size=BATCH_SIZE,\n",
    "                       epochs=1,\n",
    "                       verbose=0)\n",
    "\n",
    "        self.train_step += 1\n",
    "        if self.train_step % TARGET_UPDATE_STEPS == 0:\n",
    "            self.update_model()\n",
    "\n",
    "\n",
    "    def save(self, data):\n",
    "        \"\"\"Save model and rewards list to appropriate dir, defined at start of code.\"\"\"\n",
    "        name = f'data_{TIMESTAMP}'\n",
    "        # saving model\n",
    "        if not os.path.exists(MODEL_DIR):\n",
    "            os.makedirs(MODEL_DIR)\n",
    "        self.target_model.save_weights(MODEL_DIR + \"model.weights.h5\")\n",
    "\n",
    "        # saving results\n",
    "        if not os.path.exists(REWARD_DIR):\n",
    "            os.makedirs(REWARD_DIR)\n",
    "        filepath = os.path.join(REWARD_DIR, name + \".csv\")\n",
    "        plotpath = os.path.join(REWARD_DIR, name + \".jpg\")\n",
    "\n",
    "        plot_agent(data, plotpath)\n",
    "\n",
    "        # Save updated data\n",
    "        np.savetxt(filepath, data, delimiter=\",\")\n",
    "\n",
    "    def load(self, name):\n",
    "        \"\"\"Load previously trained model weights.\"\"\"\n",
    "        self.model.load_weights(name)\n",
    "        self.update_model()\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "8265e5e3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "id": "4f52b10c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T08:55:30.527730Z",
     "start_time": "2025-08-10T08:55:30.521444Z"
    }
   },
   "source": [
    "def train_agent(agent: DDQN_Agent, env: gym.make, episodes: int):\n",
    "    \"\"\"Train agent with experience replay, batch fitting and using a cropped greyscale input image.\"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in tqdm(range(episodes), desc=\"Training\"):\n",
    "        print(f\"[INFO] Starting Episode {episode}\")\n",
    "\n",
    "        state_colour, _ = env.reset()\n",
    "        state_gray, can_see_road, car_on_grass = convert_greyscale(state_colour)\n",
    "\n",
    "        # 4-frame grey buffer\n",
    "        frame_buffer = deque([state_gray] * 4, maxlen=4)\n",
    "\n",
    "        # Steering‐history buffer\n",
    "        steer_buffer = deque([0] * STEER_BUFFER_LEN, maxlen=STEER_BUFFER_LEN)\n",
    "\n",
    "        # Track last steer to penalize jitter\n",
    "        last_steer = 0.0\n",
    "\n",
    "        sum_reward    = 0.0\n",
    "        negative_streak = 0\n",
    "        done = False\n",
    "\n",
    "        while (not done) and (sum_reward > MAX_PENALTY) and can_see_road:\n",
    "\n",
    "            state_input = np.mean(np.stack(frame_buffer, axis=0), axis=0, keepdims=True)\n",
    "\n",
    "            # Agent picks an action\n",
    "            raw_action = agent.choose_action(state_input[0])\n",
    "            steer, gas, brake = raw_action\n",
    "            action = np.array(raw_action, dtype=np.float64)\n",
    "\n",
    "            # Record steering history & penalize rapid changes\n",
    "            steer_buffer.append(steer)\n",
    "            steer_change = abs(steer - last_steer)\n",
    "            last_steer = steer\n",
    "\n",
    "            # small penalty on jitter\n",
    "            jitter_penalty = 0.005 * steer_change\n",
    "\n",
    "            # SKIP_FRAMES times in a row.\n",
    "            reward_accum = 0.0\n",
    "            for _ in range(SKIP_FRAMES + 1):\n",
    "                next_colour, r, term, trunc, _ = env.step(action)\n",
    "                reward_accum += r\n",
    "\n",
    "                # render if user has specified, break if terminal\n",
    "                if RENDER:\n",
    "                    env.render()\n",
    "                if term or trunc:\n",
    "                    done = True\n",
    "                    break\n",
    "\n",
    "            # Count number of negative rewards collected sequentially, if reward non-negative, restart counting\n",
    "            negative_streak = (negative_streak + 1) if (reward_accum < 0) else 0\n",
    "            if negative_streak >= CONSECUTIVE_NEG_REWARD:\n",
    "                break\n",
    "\n",
    "            # convert to greyscale for NN\n",
    "            next_grey, can_see_road, car_on_grass = convert_greyscale(next_colour)\n",
    "            frame_buffer.append(next_grey)\n",
    "\n",
    "            # Use some penalties for car being on grass or other features and reshape the reward\n",
    "\n",
    "            # Grass penalty\n",
    "            if car_on_grass:\n",
    "                reward_accum -= 0.12\n",
    "\n",
    "            # predicting turns and corners\n",
    "            off_far = calculate_off_far(next_colour)\n",
    "            turniness = sum(abs(s) for s in steer_buffer) / STEER_BUFFER_LEN\n",
    "\n",
    "            if abs(off_far) < 0.2 and turniness < 0.2:\n",
    "                # straight road ahead, reward gas\n",
    "                reward_accum += 0.012 * gas\n",
    "            else:\n",
    "                # approaching orin turn, penalize gas\n",
    "                severity = max(turniness, abs(off_far))\n",
    "                reward_accum -= 0.04 * gas * severity\n",
    "\n",
    "\n",
    "            # Brake only when a curve is coming\n",
    "            if abs(off_far) > 0.3:\n",
    "\n",
    "                reward_accum += 0.015 * brake * abs(off_far)\n",
    "\n",
    "            # jitter penalty\n",
    "            reward_accum -= jitter_penalty\n",
    "\n",
    "            # Add reward clipping for meaningfull rewards\n",
    "            reward_accum = np.clip(reward_accum, -1.0, 1.0)\n",
    "\n",
    "            # store transition states for experience replay\n",
    "            prev_state = state_input[0]\n",
    "            next_state = np.mean(np.stack(frame_buffer, axis=0), axis=0)\n",
    "            action_idx = agent.action_space.index(tuple(raw_action))\n",
    "            agent.store_transition(prev_state, action_idx, reward_accum, next_state, done)\n",
    "\n",
    "            # experience replay training with a batch of data\n",
    "            agent.experience_replay()\n",
    "\n",
    "            # update params for next loop\n",
    "            sum_reward += reward_accum\n",
    "\n",
    "        # Store episode reward\n",
    "        episode_rewards.append([sum_reward, agent.epsilon])\n",
    "\n",
    "        # update target action value network every N steps ( to equal action value network)\n",
    "        print(f\"[INFO] Episode {episode} → Reward: {sum_reward:.2f} | ε: {agent.epsilon:.4f}\")\n",
    "\n",
    "        if episode % TARGET_UPDATE_STEPS == 0:\n",
    "            agent.update_model()\n",
    "        if episode % SAVE_TRAINING_FREQUENCY == 0:\n",
    "            agent.save(episode_rewards)\n",
    "\n",
    "        # Exponential ε-decay\n",
    "        agent.epsilon = max(agent.epsilon_min, agent.epsilon * EPSILON_DECAY)\n",
    "\n",
    "    env.close()\n",
    "    return episode_rewards\n"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "7d7facfa",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "id": "508e2be2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T08:55:30.536214Z",
     "start_time": "2025-08-10T08:55:30.532202Z"
    }
   },
   "source": [
    "def test_agent(agent: DDQN_Agent, env: gym.make, model: str, testnum=10):\n",
    "    \"\"\"Test a pretrained model and print out run rewards and total time taken. Quit with ctrl+c.\"\"\"\n",
    "    # Load agent model\n",
    "    agent.load(model)\n",
    "\n",
    "    run_rewards = []\n",
    "    for test in range(testnum):\n",
    "        state_colour, _ = env.reset()\n",
    "        state_grey, _, _ = convert_greyscale(state_colour)\n",
    "\n",
    "        done = False\n",
    "        sum_reward = 0.0\n",
    "        t1 = time.time()    # Trial timer\n",
    "        while not done:\n",
    "\n",
    "            # choose action to take next\n",
    "            action = agent.choose_action(state_grey, best=True)\n",
    "            action = np.array(action, dtype=np.float64)\n",
    "\n",
    "            # take action and observe new state, reward and if terminal\n",
    "            new_state_colour, r, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            # render if user has specified\n",
    "            if RENDER:\n",
    "                env.render()\n",
    "\n",
    "            # convert to greyscale for NN\n",
    "            state_grey, _, _ = convert_greyscale(new_state_colour)\n",
    "            sum_reward += r\n",
    "\n",
    "        t1 = time.time() - t1\n",
    "        run_rewards.append([sum_reward, np.nan, t1, np.nan, np.nan, np.nan])\n",
    "        print(f\"[INFO]: Run {test} | Run Reward: {sum_reward:.2f} | Time: {t1:.2f}s\")\n",
    "\n",
    "    # calculate useful statistics\n",
    "    rr = [i[0] for i in run_rewards]\n",
    "    rt = [i[2] for i in run_rewards]\n",
    "\n",
    "    r_max = max(rr)\n",
    "    r_min = min(rr)\n",
    "    r_std_dev = np.std(rr)\n",
    "    r_avg = np.mean(rr)\n",
    "    t_avg = np.mean(rt)\n",
    "\n",
    "    run_rewards.append([r_avg, np.nan, t_avg, r_max, r_min, r_std_dev])\n",
    "    print(\n",
    "        f\"[INFO]: Runs {testnum} | Avg Run Reward: {r_avg:.2f} \"\n",
    "        f\"| Avg Time: {t_avg:.2f}s | Max: {r_max:.2f} | Min: {r_min:.2f} | Std Dev: {r_std_dev:.2f}\"\n",
    "    )\n",
    "\n",
    "    # saving test results\n",
    "    if not os.path.exists(f\"test_{REWARD_DIR}\"):\n",
    "        os.makedirs(f\"test_{REWARD_DIR}\")\n",
    "    path = f\"test_{REWARD_DIR}\" + PRETRAINED_PATH.split('/')[-1][:-3] + \"_run_rewards.csv\"\n",
    "    np.savetxt(path, run_rewards, delimiter=\",\")\n",
    "\n",
    "    # return average results\n",
    "    return [r_avg, np.nan, t_avg, r_max, r_min, r_std_dev]\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "dd2951b6",
   "metadata": {},
   "source": [
    "main.py"
   ]
  },
  {
   "cell_type": "code",
   "id": "759f39e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-10T09:00:04.460037Z",
     "start_time": "2025-08-10T08:55:30.542815Z"
    }
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    if not TEST:\n",
    "        env = gym.make('CarRacing-v3')\n",
    "        # Train Agent\n",
    "        agent = DDQN_Agent()\n",
    "        train_agent( agent, env, episodes = EPISODES )\n",
    "\n",
    "    else:\n",
    "        # Test Agent\n",
    "        env = gym.make('CarRacing-v3',render_mode='human')\n",
    "        agent = DDQN_Agent()\n",
    "\n",
    "        test_agent( agent, env, model = PRETRAINED_PATH, testnum=5 )"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Run 0 | Run Reward: 911.90 | Time: 50.18s\n",
      "[INFO]: Run 1 | Run Reward: 904.10 | Time: 55.51s\n",
      "[INFO]: Run 2 | Run Reward: 868.31 | Time: 57.59s\n",
      "[INFO]: Run 3 | Run Reward: 909.10 | Time: 52.45s\n",
      "[INFO]: Run 4 | Run Reward: 857.93 | Time: 57.86s\n",
      "[INFO]: Runs 5 | Avg Run Reward: 890.27 | Avg Time: 54.72s | Max: 911.90 | Min: 857.93 | Std Dev: 22.55\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
